# ğŸ“ Question Generation System using Bloomâ€™s Taxonomy, FAISS & Gemini

This project is a smart **Question Generation System** that creates questions from lecture transcripts and course outcomes. It supports **Bloomâ€™s Taxonomy-based filtering**, integrates **semantic vector search using FAISS**, and uses **Google Gemini API** for generating questions. 

---

## ğŸš€ Technologies Used

- ğŸ§  **Google Gemini API** â€“ for question generation
- ğŸ§Š **FAISS** â€“ vector search for retrieving relevant content
- ğŸ§¾ **SentenceTransformer** â€“ for converting text into dense vectors
- âš™ï¸ **Flask** â€“ backend API to handle question generation
- ğŸŒ **Streamlit** â€“ frontend UI to interact with the model

---

## ğŸ“ Folder Structure

```
question_generation2/
â”œâ”€â”€ backend.py                  # Flask API server
â”œâ”€â”€ frontend/                   # Streamlit frontend UI
â”‚   â””â”€â”€ app.py                  # Main UI file
â”œâ”€â”€ cleaned_transcript.txt      # Transcript content file
â”œâ”€â”€ course_outcomes.txt         # Course outcomes file
â”œâ”€â”€ case_materials/             # Folder for optional case materials
â”œâ”€â”€ faiss_index.index           # FAISS vector index (auto-generated)
â”œâ”€â”€ embeddings.npy              # Stored sentence embeddings
â”œâ”€â”€ chunks.npy                  # Stored transcript chunks
â”œâ”€â”€ .env                        # API key and config variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ’¡ How It Works

### ğŸ§© Step-by-step Flow

1. **User Interface (Streamlit)**:
   - Upload optional **case material** (`.txt`)
   - Select **question type** (MCQ, Short, Long)
   - Choose **Bloomâ€™s Taxonomy levels**
   - Add optional **custom prompt**
   - Click "Generate Questions"

2. **Backend Processing (Flask)**:
   - Loads `cleaned_transcript.txt` and `course_outcomes.txt`
   - Uses SentenceTransformer to convert them into dense vectors
   - Builds a FAISS index and finds relevant content
   - Passes this content with prompt to **Google Gemini API**
   - Returns generated questions back to Streamlit

---

## âš™ï¸ Installation & Setup

### ğŸ”¹ Step 1: Clone the repository

```bash
git clone https://github.com/khyatiiisingh/question_generation2.git
cd question_generation2
```

---

### ğŸ”¹ Step 2: Set up a virtual environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Git Bash / Linux / Mac
source venv/bin/activate
```

---

### ğŸ”¹ Step 3: Install dependencies

```bash
pip install -r requirements.txt
```

---

### ğŸ”¹ Step 4: Add your Gemini API Key

Create a `.env` file in the root directory:

```env
GOOGLE_API_KEY=your_gemini_api_key_here
```

---

## â–¶ï¸ Running the App

### âœ… Step 1: Start the Flask Backend (port 5000)

```bash
python backend.py
```

Youâ€™ll see:
```
Running on http://127.0.0.1:5000
```

---

### âœ… Step 2: Start the Streamlit Frontend (port 8501)

In a new terminal:

```bash
cd frontend
streamlit run app.py
```

Visit in browser:  
ğŸ‘‰ `http://localhost:8501`

---

## ğŸ“Œ Notes

- Make sure `cleaned_transcript.txt` and `course_outcomes.txt` are present in the root folder.
- You can upload additional **case material** via the Streamlit UI as a `.txt` file.
- Large vector files like `chunks.npy`, `faiss_index.index`, and `embeddings.npy` are excluded from Git using `.gitignore`.
